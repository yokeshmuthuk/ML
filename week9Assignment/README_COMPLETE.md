# Week 9 Assignment - Complete Solution
## CS7CS4/CSU44061 Machine Learning - Transformer Models

This repository contains a complete solution framework for the Week 9 assignment on transformers and GPT models.

---

## üìÅ Files Overview

### Analysis & Utilities
- `analyze_datasets.py` - Dataset analysis (vocabulary, length, content)
- `calculate_parameters.py` - Parameter count calculator for different configurations
- `test_evaluation.py` - Test set evaluation with baseline comparisons

### Model Configurations (Part i.b-c)
- `gpt_config1.py` - Configuration 1: Aggressive reduction (0.82M params)
- `gpt_config2.py` - Configuration 2: Fewer layers, higher embedding (0.95M params)
- `gpt_config3.py` - Configuration 3: Balanced approach (0.75M params)

### Experiments
- `gpt_bias_experiment.py` - Bias terms in self-attention (Part i.d)
- `gpt_skip_experiment.py` - Skip connections impact (Part i.e)

### Documentation
- `ASSIGNMENT_GUIDE.md` - Detailed guide with expected results and interpretations
- `week9_questions.pdf` - Original assignment questions

### Data
- `input_childSpeech_trainingSet.txt` - Child speech training data
- `input_childSpeech_testSet.txt` - Child speech test data
- `input_shakespeare.txt` - Shakespeare plays (out-of-distribution test)

---

## üöÄ Quick Start

### 1. Install Dependencies
```bash
pip3 install torch --break-system-packages
```

### 2. Run Dataset Analysis
```bash
python analyze_datasets.py
```

**Key Findings:**
- Child Speech: 246K chars, 40 unique chars, simple grammar
- Shakespeare: 1.1M chars, 65 unique chars, complex language
- Vocab difference enables OOD evaluation

### 3. Train Models

```bash
# Train all three configurations
python gpt_config1.py  # ~0.82M parameters
python gpt_config2.py  # ~0.95M parameters
python gpt_config3.py  # ~0.75M parameters
```

Each produces:
- Saved model checkpoint (`model_configN.pt`)
- Training/validation loss logs
- Generated text samples

### 4. Run Experiments

**Bias Terms:**
```bash
# Edit gpt_bias_experiment.py, set USE_BIAS_IN_ATTENTION = False
python gpt_bias_experiment.py

# Edit again, set USE_BIAS_IN_ATTENTION = True
python gpt_bias_experiment.py
```

**Skip Connections:**
```bash
# Edit gpt_skip_experiment.py, set USE_SKIP_CONNECTIONS = False
python gpt_skip_experiment.py

# Edit again, set USE_SKIP_CONNECTIONS = True
python gpt_skip_experiment.py
```

### 5. Evaluate on Test Sets
```bash
python test_evaluation.py
```

This calculates baseline losses for comparison.

---

## üìä Expected Results Summary

### Part (i): Model Configurations

| Config | n_embd | n_head | n_layer | block_size | Parameters | Approach |
|--------|--------|--------|---------|------------|------------|----------|
| 1      | 128    | 4      | 4       | 128        | 0.82M      | Aggressive |
| 2      | 192    | 4      | 3       | 128        | 0.95M      | Wide & shallow |
| 3      | 160    | 4      | 3       | 128        | 0.75M      | Balanced |

**Rationales:**

**Config 1:** Primary reduction in n_embd (384‚Üí128) because feed-forward network scales quadratically with embedding dimension. Most aggressive, smallest model.

**Config 2:** Trade depth for width - fewer layers (3 vs 6) but higher embeddings (192). Hypothesis: simple child speech doesn't need deep hierarchical processing but benefits from richer representations.

**Config 3:** Balanced reduction across all dimensions. Middle ground between Config 1 (too small) and Config 2 (shallow but wide).

### Part (ii): Baseline Comparisons

**Child Speech Test (40 chars):**
- Uniform baseline: ~3.69
- Unigram baseline: ~2.5-3.0
- Expected model loss: 1.5-2.5

**Shakespeare (65 chars):**
- Uniform baseline: ~4.17
- Unigram baseline: ~2.8-3.5
- Expected loss (trained on child speech): 3.5-4.5 (high due to domain shift)

---

## üî¨ Key Concepts & Interpretations

### Dataset Characteristics
1. **Child Speech**: Limited vocabulary, simple patterns, repetitive structures
2. **Shakespeare**: Rich vocabulary, complex syntax, formal language
3. **Domain Shift**: Different char distributions, vocabulary, complexity

### Model Downsizing Strategies
1. **Embedding dimension (n_embd)**: Biggest impact - affects FFN quadratically
2. **Number of layers (n_layer)**: Linear impact - reduces depth
3. **Attention heads (n_head)**: Moderate impact - affects attention capacity
4. **Block size**: Affects context window and positional embeddings

### Bias in Self-Attention
- **With bias**: More flexible, can learn offsets to attention scores
- **Without bias**: Forced to learn purely relative patterns
- **Impact**: Usually minimal for simple tasks, helps with complex patterns

### Skip Connections
- **Critical** for training deep networks
- Enable gradient flow through many layers
- Allow learning residual functions
- **Without them**: Vanishing gradients, poor performance

### Out-of-Distribution Evaluation
- High loss on Shakespeare indicates domain mismatch
- Practical applications:
  - Anomaly detection
  - Data quality monitoring
  - Identifying when retraining is needed
  - Active learning sample selection

---

## üìù Report Writing Checklist

- [ ] Part (i.a): Describe all 3 datasets with vocab size, length, content
- [ ] Part (i.b): Config 1 with parameter count and rationale
- [ ] Part (i.c): Configs 2 & 3 with loss comparisons and discussion
- [ ] Part (i.d): Bias experiment results and interpretation
- [ ] Part (i.e): Skip connection experiment and analysis
- [ ] Part (ii.a): Test on child speech vs baselines
- [ ] Part (ii.b): Test on Shakespeare with interpretation
- [ ] All plots labeled with clear fonts
- [ ] Code appended to report
- [ ] ZIP file with executable code

---

## üí° Tips for Success

### Analysis, Not Just Numbers
Don't say: "The loss was 2.34"

Do say: "The validation loss converged to 2.34, significantly better than the unigram baseline of 3.02, indicating the model learned meaningful linguistic patterns beyond simple character frequency. Compared to Config 2's loss of 2.10, this suggests the larger embedding dimension provides better representation capacity."

### Key Discussion Points
1. Why specific parameters were reduced
2. How training/validation losses compare
3. Evidence of overfitting or underfitting
4. Quality of generated text
5. Comparison to baselines
6. Architectural impacts (bias, skip connections)
7. Practical implications

### Remember
- Explanation matters more than code
- Discuss interpretations, not just numbers
- Compare across configs and to baselines
- Explain WHY results occurred
- Keep under 10 pages (excluding appendix)

---

## üéØ Assignment Completion Status

All scripts are ready to run. To complete the assignment:

1. Run all training scripts (may take 30-60 minutes total on CPU)
2. Collect loss curves and generated text
3. Run experiments (bias, skip connections)
4. Calculate test set losses
5. Write report with analysis and interpretation
6. Append code to PDF report
7. Create ZIP with code and data

---

## üìö Additional Resources

- Original GPT paper: "Attention Is All You Need" (Vaswani et al.)
- nanoGPT repository: https://github.com/karpathy/nanoGPT
- Andrej Karpathy's "Neural Networks: Zero to Hero" series

---

## ‚ö†Ô∏è Important Notes

- Training on CPU will be slow (but feasible for 1000 iterations)
- Use `max_iters = 1000` for quick experiments
- Random seed is set (1337) for reproducibility
- Models save checkpoints automatically
- Generated text quality improves with training

---

Good luck with your assignment! üöÄ
