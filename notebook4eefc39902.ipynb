{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e59bd3b5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-28T23:04:51.815463Z",
     "iopub.status.busy": "2025-12-28T23:04:51.815210Z",
     "iopub.status.idle": "2025-12-28T23:04:53.136085Z",
     "shell.execute_reply": "2025-12-28T23:04:53.135341Z"
    },
    "papermill": {
     "duration": 1.32626,
     "end_time": "2025-12-28T23:04:53.137921",
     "exception": false,
     "start_time": "2025-12-28T23:04:51.811661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "942f6809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T23:04:53.143420Z",
     "iopub.status.busy": "2025-12-28T23:04:53.143059Z",
     "iopub.status.idle": "2025-12-28T23:05:00.184384Z",
     "shell.execute_reply": "2025-12-28T23:05:00.183665Z"
    },
    "papermill": {
     "duration": 7.045707,
     "end_time": "2025-12-28T23:05:00.185945",
     "exception": false,
     "start_time": "2025-12-28T23:04:53.140238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x789ba1ffcb30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, math, random, time, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc02c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T23:05:00.191980Z",
     "iopub.status.busy": "2025-12-28T23:05:00.191541Z",
     "iopub.status.idle": "2025-12-28T23:05:01.748276Z",
     "shell.execute_reply": "2025-12-28T23:05:01.747358Z"
    },
    "papermill": {
     "duration": 1.561698,
     "end_time": "2025-12-28T23:05:01.749838",
     "exception": false,
     "start_time": "2025-12-28T23:05:00.188140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math: 60000 8000 Bool: 60000 8000\n",
      "Sample math: (8+5)*9=117\n",
      "Sample bool: (NOT False AND True) AND NOT (False) = True\n",
      "Wrote datasets/ ملفات\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Math dataset\n",
    "# -------------------------\n",
    "def safe_eval_int(expr: str):\n",
    "    allowed = set(\"0123456789+-*/() \")\n",
    "    if any(c not in allowed for c in expr):\n",
    "        return None\n",
    "    try:\n",
    "        val = eval(expr, {\"__builtins__\": None}, {})\n",
    "        if isinstance(val, (int, float)) and abs(val - round(val)) < 1e-9:\n",
    "            return int(round(val))\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def gen_math_expr(level: int) -> str:\n",
    "    # progressively harder\n",
    "    if level == 0:\n",
    "        a, b = random.randint(0, 9), random.randint(0, 9)\n",
    "        return f\"{a}+{b}\"\n",
    "    if level == 1:\n",
    "        a, b = random.randint(0, 99), random.randint(0, 99)\n",
    "        op = random.choice([\"+\", \"-\"])\n",
    "        return f\"{a}{op}{b}\"\n",
    "    if level == 2:\n",
    "        a, b, c = random.randint(0, 9), random.randint(0, 9), random.randint(0, 9)\n",
    "        op = random.choice([\"+\", \"-\"])\n",
    "        return f\"({a}{op}{b})*{c}\"\n",
    "    # level 3: exact division\n",
    "    m = random.randint(1, 12)\n",
    "    k = random.randint(0, 80)\n",
    "    left = k * m\n",
    "    if random.random() < 0.5:\n",
    "        t = random.randint(0, 20)\n",
    "        return f\"({left}+{t}-{t})/{m}\"\n",
    "    return f\"{left}/{m}\"\n",
    "\n",
    "def make_math_dataset(n, levels=(0,1,2,3)):\n",
    "    out = []\n",
    "    while len(out) < n:\n",
    "        expr = gen_math_expr(random.choice(levels))\n",
    "        ans = safe_eval_int(expr)\n",
    "        if ans is None: \n",
    "            continue\n",
    "        out.append(f\"{expr}={ans}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Boolean dataset (NO Python eval)\n",
    "# Grammar supports: True False AND OR XOR NOT and parentheses\n",
    "# -------------------------\n",
    "def tok_bool(s):\n",
    "    s = s.replace(\"(\", \" ( \").replace(\")\", \" ) \")\n",
    "    return s.split()\n",
    "\n",
    "def parse_bool(tokens):\n",
    "    # expr := term ((OR|XOR) term)*\n",
    "    # term := factor (AND factor)*\n",
    "    # factor := NOT factor | atom\n",
    "    # atom := True | False | '(' expr ')'\n",
    "\n",
    "    def parse_atom(i):\n",
    "        if tokens[i] == \"True\":  return True, i+1\n",
    "        if tokens[i] == \"False\": return False, i+1\n",
    "        if tokens[i] == \"(\":\n",
    "            v, j = parse_expr(i+1)\n",
    "            if tokens[j] != \")\":\n",
    "                raise ValueError(\"Missing )\")\n",
    "            return v, j+1\n",
    "        raise ValueError(\"Bad atom\")\n",
    "\n",
    "    def parse_factor(i):\n",
    "        if tokens[i] == \"NOT\":\n",
    "            v, j = parse_factor(i+1)\n",
    "            return (not v), j\n",
    "        return parse_atom(i)\n",
    "\n",
    "    def parse_term(i):\n",
    "        v, j = parse_factor(i)\n",
    "        while j < len(tokens) and tokens[j] == \"AND\":\n",
    "            rhs, j2 = parse_factor(j+1)\n",
    "            v = v and rhs\n",
    "            j = j2\n",
    "        return v, j\n",
    "\n",
    "    def parse_expr(i):\n",
    "        v, j = parse_term(i)\n",
    "        while j < len(tokens) and tokens[j] in (\"OR\", \"XOR\"):\n",
    "            op = tokens[j]\n",
    "            rhs, j2 = parse_term(j+1)\n",
    "            v = (v or rhs) if op == \"OR\" else (v != rhs)\n",
    "            j = j2\n",
    "        return v, j\n",
    "\n",
    "    v, j = parse_expr(0)\n",
    "    if j != len(tokens):\n",
    "        raise ValueError(\"Extra tokens\")\n",
    "    return v\n",
    "\n",
    "def gen_bool_expr(depth):\n",
    "    OPS = [\"AND\", \"OR\", \"XOR\"]\n",
    "    LITS = [\"True\", \"False\"]\n",
    "    if depth <= 0:\n",
    "        atom = random.choice(LITS)\n",
    "        return f\"NOT {atom}\" if random.random() < 0.25 else atom\n",
    "\n",
    "    r = random.random()\n",
    "    if r < 0.25:\n",
    "        return f\"NOT ({gen_bool_expr(depth-1)})\"\n",
    "\n",
    "    left = gen_bool_expr(depth-1)\n",
    "    right = gen_bool_expr(depth-1)\n",
    "    op = random.choice(OPS)\n",
    "    if random.random() < 0.75:\n",
    "        return f\"({left} {op} {right})\"\n",
    "    return f\"{left} {op} {right}\"\n",
    "\n",
    "def make_bool_dataset(n, max_depth):\n",
    "    out = []\n",
    "    while len(out) < n:\n",
    "        d = random.randint(0, max_depth)\n",
    "        expr = gen_bool_expr(d)\n",
    "        val = parse_bool(tok_bool(expr))\n",
    "        out.append(f\"{expr} = {'True' if val else 'False'}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Build datasets (adjust sizes if needed)\n",
    "# -------------------------\n",
    "math_train = make_math_dataset(60000, levels=(0,1,1,2,2,3))\n",
    "math_test  = make_math_dataset(8000,  levels=(1,2,2,3,3))\n",
    "\n",
    "bool_train = make_bool_dataset(60000, max_depth=3)\n",
    "bool_test  = make_bool_dataset(8000,  max_depth=5)\n",
    "\n",
    "print(\"Math:\", len(math_train), len(math_test), \"Bool:\", len(bool_train), len(bool_test))\n",
    "print(\"Sample math:\", math_train[0])\n",
    "print(\"Sample bool:\", bool_train[0])\n",
    "\n",
    "# optional: write datasets to files (useful for zip submission)\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "open(\"datasets/math_train.txt\",\"w\",encoding=\"utf-8\").write(\"\\n\".join(math_train)+\"\\n\")\n",
    "open(\"datasets/math_test.txt\",\"w\",encoding=\"utf-8\").write(\"\\n\".join(math_test)+\"\\n\")\n",
    "open(\"datasets/bool_train.txt\",\"w\",encoding=\"utf-8\").write(\"\\n\".join(bool_train)+\"\\n\")\n",
    "open(\"datasets/bool_test.txt\",\"w\",encoding=\"utf-8\").write(\"\\n\".join(bool_test)+\"\\n\")\n",
    "print(\"Wrote datasets/ ملفات\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "323004cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T23:05:01.755335Z",
     "iopub.status.busy": "2025-12-28T23:05:01.755088Z",
     "iopub.status.idle": "2025-12-28T23:05:01.773164Z",
     "shell.execute_reply": "2025-12-28T23:05:01.772351Z"
    },
    "papermill": {
     "duration": 0.022701,
     "end_time": "2025-12-28T23:05:01.774768",
     "exception": false,
     "start_time": "2025-12-28T23:05:01.752067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_gpt_language_model(vocab_chars, *,\n",
    "                             block_size=128,\n",
    "                             n_embd=256,\n",
    "                             n_head=4,\n",
    "                             n_layer=4,\n",
    "                             dropout=0.1):\n",
    "    \"\"\"\n",
    "    Builds the GPTLanguageModel implementation from gpt.py,\n",
    "    but with notebook-controlled hyperparameters & vocab.\n",
    "    \"\"\"\n",
    "\n",
    "    chars = sorted(list(set(vocab_chars)))\n",
    "    vocab_size = len(chars)\n",
    "    stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "    itos = {i:ch for ch,i in stoi.items()}\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda ids: ''.join([itos[i] for i in ids])\n",
    "\n",
    "    # ---- Below is structurally the same as your gpt.py classes ----\n",
    "    class Head(nn.Module):\n",
    "        def __init__(self, head_size):\n",
    "            super().__init__()\n",
    "            self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            B,T,C = x.shape\n",
    "            k = self.key(x)\n",
    "            q = self.query(x)\n",
    "            wei = q @ k.transpose(-2,-1) * (k.shape[-1] ** -0.5)\n",
    "            wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "            wei = F.softmax(wei, dim=-1)\n",
    "            wei = self.dropout(wei)\n",
    "            v = self.value(x)\n",
    "            out = wei @ v\n",
    "            return out\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, num_heads, head_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "            self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "            out = self.dropout(self.proj(out))\n",
    "            return out\n",
    "\n",
    "    class FeedFoward(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_embd, 4*n_embd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4*n_embd, n_embd),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    class Block(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            head_size = n_embd // n_head\n",
    "            self.sa = MultiHeadAttention(n_head, head_size)\n",
    "            self.ffwd = FeedFoward()\n",
    "            self.ln1 = nn.LayerNorm(n_embd)\n",
    "            self.ln2 = nn.LayerNorm(n_embd)\n",
    "        def forward(self, x):\n",
    "            x = x + self.sa(self.ln1(x))\n",
    "            x = x + self.ffwd(self.ln2(x))\n",
    "            return x\n",
    "\n",
    "    class GPTLanguageModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "            self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "            self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "        def _init_weights(self, module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        def forward(self, idx, targets=None):\n",
    "            B, T = idx.shape\n",
    "            tok_emb = self.token_embedding_table(idx)\n",
    "            pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "            x = tok_emb + pos_emb\n",
    "            x = self.blocks(x)\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "            loss = None\n",
    "            if targets is not None:\n",
    "                B, T, C = logits.shape\n",
    "                logits_2d = logits.view(B*T, C)\n",
    "                targets_2d = targets.view(B*T)\n",
    "                loss = F.cross_entropy(logits_2d, targets_2d)\n",
    "            return logits, loss\n",
    "\n",
    "        def generate(self, idx, max_new_tokens):\n",
    "            for _ in range(max_new_tokens):\n",
    "                idx_cond = idx[:, -block_size:]\n",
    "                logits, _ = self(idx_cond)\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "            return idx\n",
    "\n",
    "    return GPTLanguageModel, encode, decode, stoi, itos, vocab_size, chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "834468ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T23:05:01.780261Z",
     "iopub.status.busy": "2025-12-28T23:05:01.780025Z",
     "iopub.status.idle": "2025-12-28T23:05:01.794224Z",
     "shell.execute_reply": "2025-12-28T23:05:01.793633Z"
    },
    "papermill": {
     "duration": 0.018722,
     "end_time": "2025-12-28T23:05:01.795637",
     "exception": false,
     "start_time": "2025-12-28T23:05:01.776915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_text(lines):\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "def train_gpt_on_lines(train_lines, test_lines, *,\n",
    "                       block_size=128, n_embd=256, n_head=4, n_layer=4, dropout=0.1,\n",
    "                       batch_size=64, max_iters=4000, eval_interval=400, learning_rate=3e-4,\n",
    "                       max_new_tokens=48):\n",
    "    # Build vocab from BOTH splits to avoid OOV\n",
    "    full_text = make_text(train_lines) + make_text(test_lines)\n",
    "    GPTClass, encode, decode, stoi, itos, vocab_size, chars = build_gpt_language_model(\n",
    "        full_text,\n",
    "        block_size=block_size, n_embd=n_embd, n_head=n_head, n_layer=n_layer, dropout=dropout\n",
    "    )\n",
    "\n",
    "    data = torch.tensor(encode(full_text), dtype=torch.long)\n",
    "    n = int(0.9 * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    def get_batch(split):\n",
    "        src = train_data if split == 'train' else val_data\n",
    "        ix = torch.randint(len(src) - block_size - 1, (batch_size,))\n",
    "        x = torch.stack([src[i:i+block_size] for i in ix])\n",
    "        y = torch.stack([src[i+1:i+block_size+1] for i in ix])\n",
    "        return x.to(device), y.to(device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(eval_iters=100):\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        for split in ['train','val']:\n",
    "            losses = []\n",
    "            for _ in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                _, loss = model(X, Y)\n",
    "                losses.append(loss.item())\n",
    "            out[split] = float(np.mean(losses))\n",
    "        model.train()\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def gen_one(prompt, max_new=max_new_tokens):\n",
    "        model.eval()\n",
    "        idx = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "        out = model.generate(idx, max_new)\n",
    "        txt = decode(out[0].tolist())\n",
    "        # take only first line (up to newline) so comparisons are stable\n",
    "        return txt.split(\"\\n\", 1)[0].strip()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_exact_match(test_lines, kind, n_samples=300):\n",
    "        # kind: \"math\" or \"bool\"\n",
    "        model.eval()\n",
    "        n_samples = min(n_samples, len(test_lines))\n",
    "        idxs = np.random.choice(len(test_lines), size=n_samples, replace=False)\n",
    "        exact_ok = 0\n",
    "        ans_ok = 0\n",
    "\n",
    "        for i in idxs:\n",
    "            gold = test_lines[i].strip()\n",
    "\n",
    "            if kind == \"math\":\n",
    "                expr, ans = gold.split(\"=\", 1)\n",
    "                prompt = f\"{expr}=\"\n",
    "                pred_line = gen_one(prompt, max_new=32)\n",
    "                # exact line\n",
    "                exact_ok += int(pred_line == gold)\n",
    "                # answer-only\n",
    "                if \"=\" in pred_line:\n",
    "                    pred_ans = pred_line.split(\"=\",1)[1].strip()\n",
    "                    ans_ok += int(pred_ans == ans.strip())\n",
    "            else:\n",
    "                left, right = gold.split(\"=\", 1)\n",
    "                prompt = f\"{left.strip()} =\"\n",
    "                pred_line = gen_one(prompt, max_new=48)\n",
    "                exact_ok += int(pred_line == gold)\n",
    "                if \"=\" in pred_line:\n",
    "                    pred_rhs = pred_line.split(\"=\",1)[1].strip()\n",
    "                    ans_ok += int(pred_rhs == right.strip())\n",
    "\n",
    "        return {\n",
    "            \"exact_match\": exact_ok / n_samples,\n",
    "            \"answer_match\": ans_ok / n_samples\n",
    "        }\n",
    "\n",
    "    model = GPTClass().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    history = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for it in tqdm(range(1, max_iters+1), desc=\"training\"):\n",
    "        if it % eval_interval == 0 or it == 1:\n",
    "            losses = estimate_loss(eval_iters=80)\n",
    "            history.append({\"iter\": it, **losses})\n",
    "            print(f\"iter {it}: train {losses['train']:.4f}  val {losses['val']:.4f}\")\n",
    "\n",
    "        xb, yb = get_batch('train')\n",
    "        _, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Training seconds:\", round(time.time()-t0,2))\n",
    "    return model, encode, decode, history, eval_exact_match, gen_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab87a64c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T23:05:01.800727Z",
     "iopub.status.busy": "2025-12-28T23:05:01.800451Z",
     "iopub.status.idle": "2025-12-28T23:10:32.623266Z",
     "shell.execute_reply": "2025-12-28T23:10:32.622461Z"
    },
    "papermill": {
     "duration": 330.827251,
     "end_time": "2025-12-28T23:10:32.624986",
     "exception": false,
     "start_time": "2025-12-28T23:05:01.797735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1: train 2.9639  val 2.9617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  10%|█         | 302/3000 [00:26<19:21,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 300: train 1.4306  val 1.4056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  20%|██        | 602/3000 [00:48<17:54,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 600: train 1.2951  val 1.2609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  30%|███       | 902/3000 [01:12<16:17,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 900: train 1.1969  val 1.1586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  40%|████      | 1202/3000 [01:36<14:32,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1200: train 1.1037  val 1.0828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  50%|█████     | 1502/3000 [02:01<11:53,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1500: train 1.0722  val 1.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  60%|██████    | 1802/3000 [02:25<09:21,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1800: train 1.0573  val 1.0255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  70%|███████   | 2102/3000 [02:49<07:06,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2100: train 1.0466  val 1.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  80%|████████  | 2402/3000 [03:13<04:45,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2400: train 1.0432  val 1.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  90%|█████████ | 2702/3000 [03:37<02:21,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2700: train 1.0339  val 1.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 3000/3000 [04:01<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000: train 1.0337  val 1.0066\n",
      "Training seconds: 241.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: model_weights_part1.pth\n",
      "Math metrics: {'exact_match': 0.8775, 'answer_match': 0.8775}\n",
      "Wrote: prompt_outputs_math.txt, metrics_math.json\n"
     ]
    }
   ],
   "source": [
    "# Reasonable starter hyperparams (tune for your architecture experiments in the report)\n",
    "math_model, math_encode, math_decode, math_hist, math_eval_fn, math_gen_one = train_gpt_on_lines(\n",
    "    math_train, math_test,\n",
    "    block_size=64, n_embd=256, n_head=4, n_layer=4, dropout=0.1,\n",
    "    batch_size=128, max_iters=3000, eval_interval=300, learning_rate=3e-4,\n",
    "    max_new_tokens=32\n",
    ")\n",
    "\n",
    "# Save weights with required filename\n",
    "torch.save(math_model.state_dict(), \"model_weights_part1.pth\")\n",
    "print(\"Saved: model_weights_part1.pth\")\n",
    "\n",
    "# Evaluate\n",
    "math_metrics = math_eval_fn(math_test, kind=\"math\", n_samples=400)\n",
    "print(\"Math metrics:\", math_metrics)\n",
    "\n",
    "# Prompt-output appendix examples (strengths & weaknesses)\n",
    "math_prompts = [\n",
    "    \"3+2=\",\n",
    "    \"47+38=\",\n",
    "    \"(3+2)*4=\",\n",
    "    \"12-5=\",\n",
    "    \"20/4=\",\n",
    "    \"(9-3)*7=\",\n",
    "    \"(10+5)/5=\",\n",
    "]\n",
    "with open(\"prompt_outputs_math.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in math_prompts:\n",
    "        out = math_gen_one(p, max_new=32)\n",
    "        f.write(f\"PROMPT: {p}\\nOUTPUT: {out}\\n---\\n\")\n",
    "\n",
    "with open(\"metrics_math.json\",\"w\") as f:\n",
    "    json.dump({\"history\": math_hist, \"final_metrics\": math_metrics}, f, indent=2)\n",
    "\n",
    "print(\"Wrote: prompt_outputs_math.txt, metrics_math.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d4038b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T23:10:32.738211Z",
     "iopub.status.busy": "2025-12-28T23:10:32.737786Z",
     "iopub.status.idle": "2025-12-28T23:19:05.115076Z",
     "shell.execute_reply": "2025-12-28T23:19:05.114248Z"
    },
    "papermill": {
     "duration": 512.434795,
     "end_time": "2025-12-28T23:19:05.116733",
     "exception": false,
     "start_time": "2025-12-28T23:10:32.681938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 1/3000 [00:06<5:08:54,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1: train 3.0104  val 3.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  10%|█         | 301/3000 [00:45<1:04:06,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 300: train 0.3218  val 0.3599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  20%|██        | 601/3000 [01:23<55:43,  1.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 600: train 0.3001  val 0.3382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  30%|███       | 901/3000 [02:02<49:53,  1.43s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 900: train 0.2942  val 0.3312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  40%|████      | 1201/3000 [02:40<42:03,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1200: train 0.2907  val 0.3308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  50%|█████     | 1501/3000 [03:18<35:25,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1500: train 0.2911  val 0.3288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  60%|██████    | 1801/3000 [03:57<28:21,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1800: train 0.2885  val 0.3260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  70%|███████   | 2101/3000 [04:35<21:09,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2100: train 0.2867  val 0.3296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  80%|████████  | 2401/3000 [05:13<14:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2400: train 0.2869  val 0.3247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  90%|█████████ | 2701/3000 [05:52<07:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2700: train 0.2863  val 0.3205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 3000/3000 [06:30<00:00,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000: train 0.2844  val 0.3267\n",
      "Training seconds: 390.28\n",
      "Saved: model_weights_part2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bool metrics: {'exact_match': 0.59, 'answer_match': 0.59}\n",
      "Wrote: prompt_outputs_bool.txt, metrics_bool.json\n"
     ]
    }
   ],
   "source": [
    "bool_model, bool_encode, bool_decode, bool_hist, bool_eval_fn, bool_gen_one = train_gpt_on_lines(\n",
    "    bool_train, bool_test,\n",
    "    block_size=96, n_embd=256, n_head=4, n_layer=4, dropout=0.1,\n",
    "    batch_size=128, max_iters=3000, eval_interval=300, learning_rate=3e-4,\n",
    "    max_new_tokens=48\n",
    ")\n",
    "\n",
    "torch.save(bool_model.state_dict(), \"model_weights_part2.pth\")\n",
    "print(\"Saved: model_weights_part2.pth\")\n",
    "\n",
    "bool_metrics = bool_eval_fn(bool_test, kind=\"bool\", n_samples=400)\n",
    "print(\"Bool metrics:\", bool_metrics)\n",
    "\n",
    "bool_prompts = [\n",
    "    \"True AND False =\",\n",
    "    \"NOT True =\",\n",
    "    \"(True OR False) AND True =\",\n",
    "    \"True XOR True =\",\n",
    "    \"NOT (False OR False) =\",\n",
    "    \"(True XOR False) AND (False OR True) =\",\n",
    "]\n",
    "with open(\"prompt_outputs_bool.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in bool_prompts:\n",
    "        out = bool_gen_one(p, max_new=48)\n",
    "        f.write(f\"PROMPT: {p}\\nOUTPUT: {out}\\n---\\n\")\n",
    "\n",
    "with open(\"metrics_bool.json\",\"w\") as f:\n",
    "    json.dump({\"history\": bool_hist, \"final_metrics\": bool_metrics}, f, indent=2)\n",
    "\n",
    "print(\"Wrote: prompt_outputs_bool.txt, metrics_bool.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbae199e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T23:19:05.450917Z",
     "iopub.status.busy": "2025-12-28T23:19:05.450307Z",
     "iopub.status.idle": "2025-12-28T23:19:05.562897Z",
     "shell.execute_reply": "2025-12-28T23:19:05.562105Z"
    },
    "papermill": {
     "duration": 0.281482,
     "end_time": "2025-12-28T23:19:05.564560",
     "exception": false,
     "start_time": "2025-12-28T23:19:05.283078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity loading test (math):\n",
      "Loaded model_weights_part1.pth OK\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate required loading behavior (Part 1 as example)\n",
    "# Note: GPTLanguageModel() requires the same vocab/hyperparams used at training time.\n",
    "# In your submission zip, you'll include this notebook/script that rebuilds the same model config + vocab.\n",
    "\n",
    "print(\"Sanity loading test (math):\")\n",
    "# rebuild math class exactly (same dataset text/vocab and hyperparams)\n",
    "full_text_math = make_text(math_train) + make_text(math_test)\n",
    "GPTMathClass, *_ = build_gpt_language_model(\n",
    "    full_text_math, block_size=64, n_embd=256, n_head=4, n_layer=4, dropout=0.1\n",
    ")\n",
    "\n",
    "m = GPTMathClass().to(device)\n",
    "m.load_state_dict(torch.load(\"model_weights_part1.pth\", map_location=device))\n",
    "m.eval()\n",
    "print(\"Loaded model_weights_part1.pth OK\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 861.212416,
   "end_time": "2025-12-28T23:19:09.302646",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-28T23:04:48.090230",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
